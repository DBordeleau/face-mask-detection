{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28f86fea-850b-4ca9-a51a-753f53479fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class MaskClassificationCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # Output: 32 x 64 x 64\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # Output: 64 x 64 x 64\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Output: 64 x 32 x 32\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            # Layer 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # Output: 128 x 32 x 32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),  # Output: 128 x 32 x 32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Output: 128 x 16 x 16\n",
    "            nn.Dropout(0.6),\n",
    "\n",
    "            # Layer 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # Output: 256 x 16 x 16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Output: 256 x 16 x 16\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Output: 256 x 8 x 8\n",
    "            nn.Dropout(0.6),\n",
    "\n",
    "            # Flatten and Fully Connected Layers\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 16 * 16, 1024),  # Input size for Linear layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 3)  # Output: 3 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        for layer in self.network:\n",
    "            xb = layer(xb)\n",
    "        return xb\n",
    "\n",
    "root = pathlib.Path('Dataset/')\n",
    "classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "\n",
    "def prediction(img_path,transformer):\n",
    "    \n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image_tensor = transformer(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor = image_tensor.cuda()\n",
    "        \n",
    "    input = Variable(image_tensor)\n",
    "    output = model(input)\n",
    "    inde = output.data.numpy().argmax()\n",
    "    pred = classes[index]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7164f9f0-a069-4620-bc43-36270efc81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "base_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),  # Mild color changes\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Small translations (no rotation)\n",
    "    transforms.RandomResizedCrop(128, scale=(0.9, 1.1), ratio=(0.9, 1.1)),  # Random cropping and scaling\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.2),  # Slight blur occasionally\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "incorrect_mask_augmentations = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "])\n",
    "\n",
    "initial_size=(128, 128)\n",
    "final_size=(128, 128)\n",
    "\n",
    "class CustomAugmentation:\n",
    "    def __init__(self, incorrect_mask_augmentations, initial_size, final_size, base_transforms, incorrect_class_idx=0):\n",
    "        self.incorrect_mask_augmentations = incorrect_mask_augmentations\n",
    "        self.incorrect_class_idx = incorrect_class_idx\n",
    "        self.base_transforms = base_transforms\n",
    "        self.resize_size = initial_size\n",
    "\n",
    "    def update_resize(self, final_size):\n",
    "        self.resize_size = final_size\n",
    "\n",
    "    def __call__(self, img, label):\n",
    "        # Convert to PIL Image if img is a tensor\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = transforms.ToPILImage()(img)\n",
    "\n",
    "        # Apply resizing to the current size\n",
    "        img = transforms.Resize(self.resize_size)(img)\n",
    "\n",
    "        # Apply specific augmentations for \"incorrectly worn mask\" class\n",
    "        if label == self.incorrect_class_idx:\n",
    "            img = self.incorrect_mask_augmentations(img)\n",
    "\n",
    "        # Apply base transforms and convert to tensor\n",
    "        img = self.base_transforms(img)        \n",
    "        return img\n",
    "\n",
    "# Initialize CustomAugmentation with initial resizing\n",
    "transform = CustomAugmentation(\n",
    "    incorrect_mask_augmentations=incorrect_mask_augmentations,\n",
    "    initial_size=(128, 128),\n",
    "    final_size=(128, 128),\n",
    "    base_transforms=base_transforms,\n",
    "    incorrect_class_idx=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da8bbaf3-50e0-4a3b-8ff6-ef87c8ce7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_dir = \"Dataset\" # loading training data\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(data_set_dir, transform=base_transforms)\n",
    "\n",
    "test_len = 982\n",
    "train_len = 8000\n",
    "train_data, test_data = random_split(dataset, [train_len, test_len])\n",
    "\n",
    "train_dataLoader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_dataLoader = DataLoader(train_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bda1690-6676-475c-822d-46d304265092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.6240\n",
      "Test Accuracy: 76.33%, Avg_Test_Loss: 0.44624\n",
      "Epoch [2/15], Loss: 0.4375\n",
      "Test Accuracy: 77.97%, Avg_Test_Loss: 0.42447\n",
      "Epoch [3/15], Loss: 0.3702\n",
      "Test Accuracy: 83.79%, Avg_Test_Loss: 0.33800\n",
      "Epoch [4/15], Loss: 0.3190\n",
      "Test Accuracy: 89.75%, Avg_Test_Loss: 0.23315\n",
      "Epoch [5/15], Loss: 0.2757\n",
      "Test Accuracy: 86.59%, Avg_Test_Loss: 0.23259\n",
      "Epoch [6/15], Loss: 0.2713\n",
      "Test Accuracy: 90.75%, Avg_Test_Loss: 0.20075\n",
      "Epoch [7/15], Loss: 0.2401\n",
      "Test Accuracy: 90.88%, Avg_Test_Loss: 0.18635\n",
      "Epoch [8/15], Loss: 0.2207\n",
      "Test Accuracy: 93.29%, Avg_Test_Loss: 0.15068\n",
      "Epoch [9/15], Loss: 0.2274\n",
      "Test Accuracy: 93.31%, Avg_Test_Loss: 0.15687\n",
      "Epoch [10/15], Loss: 0.2071\n",
      "Test Accuracy: 93.36%, Avg_Test_Loss: 0.13733\n",
      "Epoch [11/15], Loss: 0.1917\n",
      "Test Accuracy: 94.15%, Avg_Test_Loss: 0.14771\n",
      "Epoch [12/15], Loss: 0.1930\n",
      "Test Accuracy: 92.12%, Avg_Test_Loss: 0.15029\n",
      "Epoch [13/15], Loss: 0.1764\n",
      "Test Accuracy: 94.94%, Avg_Test_Loss: 0.11263\n",
      "Epoch [14/15], Loss: 0.1552\n",
      "Test Accuracy: 94.90%, Avg_Test_Loss: 0.11049\n",
      "Epoch [15/15], Loss: 0.1466\n",
      "Test Accuracy: 95.41%, Avg_Test_Loss: 0.09503\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_weights = torch.tensor([3.0, 1.0, 1.0], device=device) \n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "model = MaskClassificationCNN()\n",
    "\n",
    "# Move the model to the appropriate device if using GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_of_epoch = 15\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "patience = 4\n",
    "\n",
    "# Initialize the optimizer with weight decay\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Learning rate scheduler to reduce the learning rate when loss is plateauing\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=4, factor=0.5)\n",
    "\n",
    "history = [] \n",
    "best_val_loss = float('inf')  \n",
    "epochs_without_improvement = 0  \n",
    "\n",
    "for epoch in range(num_of_epoch):\n",
    "    model.train()  # Set model to training mode\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_dataLoader:\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to the device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = loss_fn(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataLoader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_of_epoch}], Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataLoader:\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to the device\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_dataLoader)\n",
    "    # Adjust learning rate\n",
    "    scheduler.step(avg_val_loss)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%, Avg_Test_Loss: {avg_val_loss:.5f}\")\n",
    "\n",
    "    # Check if the validation loss improved for early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss \n",
    "        epochs_without_improvement = 0  \n",
    "        # Save model whenever the best loss improves\n",
    "        scripted_model = torch.jit.script(model)\n",
    "        scripted_model.save(\"model_v3_scripted.pth\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # Check for early stopping\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "        break\n",
    "\n",
    "    # Log training history\n",
    "    history.append({'epoch': epoch+1, 'train_loss': avg_train_loss, 'test_accuracy': accuracy})\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bb3fad-acef-4ab2-81d2-a0022263dc75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
